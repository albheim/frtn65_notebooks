{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ex6_boosting.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/albheim/frtn65_notebooks/blob/master/ex6_boosting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgHjPlk9tuSf"
      },
      "source": [
        "# Import modules\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Dg1JnKZtoXa"
      },
      "source": [
        "# Download dataset from url, save it to sonar.csv\n",
        "!wget -O sonar.csv https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BStGbJPXuXEL"
      },
      "source": [
        "data = pd.read_csv(\"sonar.csv\", header=None)\n",
        "data.iloc[:, 60] = 1 * (data.iloc[:, 60] == 'M') # Replace labels M and R with 1 and 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9DT1gdXvUTY"
      },
      "source": [
        "feature_selection = range(0, 60) # All features\n",
        "#feature_selection = [1, 2, 3, 4, 5] # Only a few features\n",
        "\n",
        "# Split data into features and labels, convert labels R->0 and M->1\n",
        "X, y = data.iloc[:, feature_selection].values, data.iloc[:, 60].values\n",
        "\n",
        "# Split into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciMDyKfD62GI"
      },
      "source": [
        "**Write your own boosting loop**\n",
        "\n",
        "You should now attempt to write your own boosting classifier using `DecisionTreeClassifier` as the base classifier in an ensamble. \n",
        "\n",
        "The final ensamble will have a weighted voting based on the accuracy of each single classifier.\n",
        "\n",
        "The steps needed are:\n",
        "\n",
        "1. **Choose a weak classifier as base**   \n",
        "2. Initialize weights for each sample\n",
        "3. **For T rounds**\n",
        "    1. Normalize the weights\n",
        "    2. For available features from the set, train a classifier using a single feature and evaluate the weighted training error\n",
        "    3. Choose the classifier with the lowest error\n",
        "    4. **Update the weights of the training samples: increase if classified wrongly by this classifier, decrease if correctly**\n",
        "4. Form the final strong classifier as the linear combination of the T classifiers with coefficient correlating to how good they are as individual classifiers\n",
        "\n",
        "If we are using a `DecisionTreeClassifier` with `max_features=None`, meaning that all features are considered for use in the split, we get step 2.2 and 2.3 for free. \n",
        "\n",
        "Training using weights can be done using the keyword `sample_weight` when calling `fit` in sklearn.\n",
        "\n",
        "Your task is to try out different settings for our base classifier and try different number of iterations when learning. You should also implement some update for the weight vector where if we predict correct on sample $i$ then $w_i$ should decrease while if we predict wrong $w_i$ should increase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIYSAtCfRc0b"
      },
      "source": [
        "classifiers = []\n",
        "coeff = []\n",
        "\n",
        "# Initially all samples are weighted the same\n",
        "w = np.ones(len(y_train))\n",
        "\n",
        "n_iterations =  # Try out different numbers \n",
        "\n",
        "for i in range(1, n_iterations+1):\n",
        "    clf = DecisionTreeClassifier(\n",
        "        # Try different parameters to the classifier\n",
        "    )\n",
        "\n",
        "    w = w / np.sum(w) # Normalize weights\n",
        "    clf.fit(X_train, y_train, sample_weight=w)\n",
        "\n",
        "    w =  # Use suitable update based on correct/incorrect classifications on training data\n",
        "\n",
        "    classifiers.append(clf)\n",
        "    coeff.append(clf.score(X_train, y_train)) # We use accuracy as ensamble coefficient\n",
        "\n",
        "    # Find the ensamble prediction and evaluate the accuracy after adding each new classifier\n",
        "    y_hat = sum(coeff[i] * classifiers[i].predict(X_test) for i in range(len(coeff))) / sum(coeff)\n",
        "    score = np.mean(np.round(y_hat) == y_test)\n",
        "    print(\"{} classifiers: test accuracy {}\".format(i, score))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNJtPFyl7ak-"
      },
      "source": [
        "**Compare to `AdaBoostClassifier`**\n",
        "\n",
        "We evaluate an `AdaBoostClassifier` in a similar fashion and print the error for each number of classifiers as in the previous cell. Do you get similar performance for both of them?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ee7EFJUsRZw"
      },
      "source": [
        "for i in range(1, n_iterations+1):\n",
        "    clf = AdaBoostClassifier(n_estimators=i,learning_rate=1)\n",
        "    clf.fit(X_train, y_train)\n",
        "    score = clf.score(X_test, y_test)\n",
        "    print(\"{} classifiers: test accuracy {}\".format(i, score))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZRGMWy26eYT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}