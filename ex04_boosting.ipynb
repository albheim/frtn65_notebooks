{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZgHjPlk9tuSf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/mgn/miniconda3/envs/dsml-py3.9/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
            "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
            "/home/mgn/miniconda3/envs/dsml-py3.9/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.2' currently installed).\n",
            "  from pandas.core import (\n"
          ]
        }
      ],
      "source": [
        "# Import modules\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-Dg1JnKZtoXa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-09-18 10:15:30--  https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified\n",
            "Saving to: ‘sonar.csv’\n",
            "\n",
            "sonar.csv               [  <=>               ]  85,72K   214KB/s    in 0,4s    \n",
            "\n",
            "2024-09-18 10:15:36 (214 KB/s) - ‘sonar.csv’ saved [87776]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download dataset from url, save it to sonar.csv\n",
        "!wget -O sonar.csv https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "BStGbJPXuXEL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape: (208, 61)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>57</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "      <th>60</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0200</td>\n",
              "      <td>0.0371</td>\n",
              "      <td>0.0428</td>\n",
              "      <td>0.0207</td>\n",
              "      <td>0.0954</td>\n",
              "      <td>0.0986</td>\n",
              "      <td>0.1539</td>\n",
              "      <td>0.1601</td>\n",
              "      <td>0.3109</td>\n",
              "      <td>0.2111</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0027</td>\n",
              "      <td>0.0065</td>\n",
              "      <td>0.0159</td>\n",
              "      <td>0.0072</td>\n",
              "      <td>0.0167</td>\n",
              "      <td>0.0180</td>\n",
              "      <td>0.0084</td>\n",
              "      <td>0.0090</td>\n",
              "      <td>0.0032</td>\n",
              "      <td>R</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0453</td>\n",
              "      <td>0.0523</td>\n",
              "      <td>0.0843</td>\n",
              "      <td>0.0689</td>\n",
              "      <td>0.1183</td>\n",
              "      <td>0.2583</td>\n",
              "      <td>0.2156</td>\n",
              "      <td>0.3481</td>\n",
              "      <td>0.3337</td>\n",
              "      <td>0.2872</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0084</td>\n",
              "      <td>0.0089</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>0.0094</td>\n",
              "      <td>0.0191</td>\n",
              "      <td>0.0140</td>\n",
              "      <td>0.0049</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0044</td>\n",
              "      <td>R</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0262</td>\n",
              "      <td>0.0582</td>\n",
              "      <td>0.1099</td>\n",
              "      <td>0.1083</td>\n",
              "      <td>0.0974</td>\n",
              "      <td>0.2280</td>\n",
              "      <td>0.2431</td>\n",
              "      <td>0.3771</td>\n",
              "      <td>0.5598</td>\n",
              "      <td>0.6194</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0232</td>\n",
              "      <td>0.0166</td>\n",
              "      <td>0.0095</td>\n",
              "      <td>0.0180</td>\n",
              "      <td>0.0244</td>\n",
              "      <td>0.0316</td>\n",
              "      <td>0.0164</td>\n",
              "      <td>0.0095</td>\n",
              "      <td>0.0078</td>\n",
              "      <td>R</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0100</td>\n",
              "      <td>0.0171</td>\n",
              "      <td>0.0623</td>\n",
              "      <td>0.0205</td>\n",
              "      <td>0.0205</td>\n",
              "      <td>0.0368</td>\n",
              "      <td>0.1098</td>\n",
              "      <td>0.1276</td>\n",
              "      <td>0.0598</td>\n",
              "      <td>0.1264</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0121</td>\n",
              "      <td>0.0036</td>\n",
              "      <td>0.0150</td>\n",
              "      <td>0.0085</td>\n",
              "      <td>0.0073</td>\n",
              "      <td>0.0050</td>\n",
              "      <td>0.0044</td>\n",
              "      <td>0.0040</td>\n",
              "      <td>0.0117</td>\n",
              "      <td>R</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0762</td>\n",
              "      <td>0.0666</td>\n",
              "      <td>0.0481</td>\n",
              "      <td>0.0394</td>\n",
              "      <td>0.0590</td>\n",
              "      <td>0.0649</td>\n",
              "      <td>0.1209</td>\n",
              "      <td>0.2467</td>\n",
              "      <td>0.3564</td>\n",
              "      <td>0.4459</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0031</td>\n",
              "      <td>0.0054</td>\n",
              "      <td>0.0105</td>\n",
              "      <td>0.0110</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0072</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>0.0107</td>\n",
              "      <td>0.0094</td>\n",
              "      <td>R</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 61 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       0       1       2       3       4       5       6       7       8   \\\n",
              "0  0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109   \n",
              "1  0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   \n",
              "2  0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   \n",
              "3  0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   \n",
              "4  0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   \n",
              "\n",
              "       9   ...      51      52      53      54      55      56      57  \\\n",
              "0  0.2111  ...  0.0027  0.0065  0.0159  0.0072  0.0167  0.0180  0.0084   \n",
              "1  0.2872  ...  0.0084  0.0089  0.0048  0.0094  0.0191  0.0140  0.0049   \n",
              "2  0.6194  ...  0.0232  0.0166  0.0095  0.0180  0.0244  0.0316  0.0164   \n",
              "3  0.1264  ...  0.0121  0.0036  0.0150  0.0085  0.0073  0.0050  0.0044   \n",
              "4  0.4459  ...  0.0031  0.0054  0.0105  0.0110  0.0015  0.0072  0.0048   \n",
              "\n",
              "       58      59  60  \n",
              "0  0.0090  0.0032   R  \n",
              "1  0.0052  0.0044   R  \n",
              "2  0.0095  0.0078   R  \n",
              "3  0.0040  0.0117   R  \n",
              "4  0.0107  0.0094   R  \n",
              "\n",
              "[5 rows x 61 columns]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load data into a pandas dataframe and take a quick look\n",
        "data = pd.read_csv(\"sonar.csv\", header=None)\n",
        "print(\"Shape:\", data.shape)\n",
        "data.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "R9DT1gdXvUTY"
      },
      "outputs": [],
      "source": [
        "feature_selection = range(0, 60) # All features\n",
        "#feature_selection = [1, 2, 3, 4, 5] # Only a few features\n",
        "\n",
        "# Split data into features and labels, convert labels R->0 and M->1\n",
        "data.iloc[:, 60] = 1 * (data.iloc[:, 60] == 'M') # Replace labels M and R with 1 and 0\n",
        "X, y = data.iloc[:, feature_selection].values, data.iloc[:, 60].values.astype(int)\n",
        "\n",
        "# Split into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dtype('int64')"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y.dtype"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciMDyKfD62GI"
      },
      "source": [
        "**Write your own boosting loop**\n",
        "\n",
        "You should now attempt to write your own boosting classifier using `DecisionTreeClassifier` as the base classifier in an ensamble. In this example, the weighted voting of the final ensamble will be based on the *accuracy* of each single classifier. You can use the information below to fill in the missing parts of the code.\n",
        "\n",
        "The steps of the algorithm are:\n",
        "\n",
        "1. **Choose a weak classifier as base**   \n",
        "2. Initialize weights for each sample\n",
        "3. **For T rounds**\n",
        "    1. Normalize the weights\n",
        "    2. For available features from the set, train a classifier using a single feature and evaluate the weighted training error\n",
        "    3. Choose the classifier with the lowest error\n",
        "    4. **Update the weights of the training samples: increase if classified wrongly by this classifier, decrease if classified correctly**\n",
        "4. Form the final strong classifier as the linear combination of the T classifiers with coefficient correlating to how good they are as individual classifiers\n",
        "\n",
        "We will use decision-trees with `max_depth=1` as the week classifiers. These can be constructed with the `DecisionTreeClassifier` from sklearn. If `DecisionTreeClassifier` uses `max_features=None`, all features are considered for use in the split. With this setup we get steps 3.2 and 3.3 for free. Training using weights can be done using the keyword `sample_weight` when calling `fit` in sklearn.\n",
        "\n",
        "Your task is to try out different settings for our base classifier and try different number of iterations when learning. You should also implement some update for the weight vector where if we predict correct on sample $i$ then $w_i$ should decrease while if we predict wrong $w_i$ should increase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "qIYSAtCfRc0b"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (3766071600.py, line 17)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_116818/3766071600.py\"\u001b[0;36m, line \u001b[0;32m17\u001b[0m\n\u001b[0;31m    w =  # Use suitable update based on correct/incorrect classifications on training data\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "classifiers = []\n",
        "coeff = []\n",
        "\n",
        "# Initially all samples are weighted the same\n",
        "w = np.ones(len(y_train))\n",
        "\n",
        "n_iterations = 100\n",
        "\n",
        "for i in range(1, n_iterations+1):\n",
        "    clf = DecisionTreeClassifier(\n",
        "        # Try different parameters to the classifier\n",
        "    )\n",
        "\n",
        "    w = w / np.sum(w) # Normalize weights\n",
        "    clf.fit(X_train, y_train, sample_weight=w)\n",
        "\n",
        "    w =  # Use suitable update based on correct/incorrect classifications on training data\n",
        "\n",
        "    classifiers.append(clf)\n",
        "    coeff.append(clf.score(X_train, y_train)) # We use accuracy as ensamble coefficient\n",
        "\n",
        "    # Find the ensamble prediction and evaluate the accuracy after adding each new classifier\n",
        "    y_hat = sum(coeff[i] * classifiers[i].predict(X_test) for i in range(len(coeff))) / sum(coeff)\n",
        "    score = np.mean(np.round(y_hat) == y_test)\n",
        "    print(\"{}-classifier ensamble: test accuracy {}\".format(i, score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNJtPFyl7ak-"
      },
      "source": [
        "**Compare to `AdaBoostClassifier`**\n",
        "\n",
        "We evaluate an `AdaBoostClassifier` in a similar fashion and print the error for each number of classifiers as in the previous cell. Do you get similar performance for both of them?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ee7EFJUsRZw"
      },
      "outputs": [],
      "source": [
        "for i in range(1, n_iterations+1):\n",
        "    clf = AdaBoostClassifier(n_estimators=i,learning_rate=1)\n",
        "    clf.fit(X_train, y_train)\n",
        "    score = clf.score(X_test, y_test)\n",
        "    print(\"{} classifiers: test accuracy {}\".format(i, score))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZRGMWy26eYT"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "ex6_boosting.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
